{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cellule 1: Import des bibliothèques nécessaires\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rasterio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUQTR\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHiver 2025\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTélédétection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTP3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTR_clip.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Ouvrir le fichier pour l'analyse\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43mrasterio\u001b[49m\u001b[38;5;241m.\u001b[39mopen(input_file)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Informations sur l'image\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInformations sur l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(input_file)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rasterio' is not defined"
     ]
    }
   ],
   "source": [
    "# Cellule 2: Sélection et chargement du fichier TIF\n",
    "# Remplacez ce chemin par votre propre fichier TIF\n",
    "input_file = r\"D:\\UQTR\\Hiver 2025\\Télédétection\\TP3\\TR_clip.tif\"\n",
    "\n",
    "# Ouvrir le fichier pour l'analyse\n",
    "src = rasterio.open(input_file)\n",
    "\n",
    "# Informations sur l'image\n",
    "print(f\"Informations sur l'image {os.path.basename(input_file)}:\")\n",
    "print(f\"Dimensions: {src.width} x {src.height} pixels\")\n",
    "print(f\"Nombre total de bandes: {src.count}\")\n",
    "print(f\"Type de données: {src.dtypes[0]}\")\n",
    "if src.crs:\n",
    "    print(f\"Système de coordonnées: {src.crs.to_string()}\")\n",
    "\n",
    "# Afficher la liste des bandes\n",
    "print(\"\\nBandes disponibles:\")\n",
    "for i in range(1, src.count + 1):\n",
    "    desc = src.descriptions[i-1] if src.descriptions and i-1 < len(src.descriptions) else \"Non spécifiée\"\n",
    "    print(f\"  Bande {i}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cellule: Visualisation des bandes sélectionnées avec barres de couleur réduites\n",
    "\n",
    "print(\"Visualisation des bandes sélectionnées...\")\n",
    "\n",
    "# Définir les indices de bandes à exclure (indices basés sur 1)\n",
    "BANDS_TO_EXCLUDE = [10, 14, 15, 16, 17, 18]\n",
    "\n",
    "# Créer une liste des indices de bandes à afficher (basés sur 1)\n",
    "bands_to_display = [i+1 for i in range(src.count) if i+1 not in BANDS_TO_EXCLUDE]\n",
    "n_bands_to_display = len(bands_to_display)\n",
    "\n",
    "print(f\"Affichage de {n_bands_to_display} bandes sur {src.count} (bandes {BANDS_TO_EXCLUDE} exclues)\")\n",
    "\n",
    "# Calculer une disposition de grille adaptée au nombre de bandes\n",
    "import math\n",
    "n_cols = min(4, n_bands_to_display)  # Maximum 4 colonnes\n",
    "n_rows = math.ceil(n_bands_to_display / n_cols)\n",
    "\n",
    "# Créer la figure avec un layout adapté\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "# Aplatir les axes si plusieurs lignes et colonnes\n",
    "if n_rows > 1 and n_cols > 1:\n",
    "    axes = axes.flatten()\n",
    "elif n_rows == 1 and n_cols > 1:\n",
    "    axes = axes\n",
    "elif n_cols == 1 and n_rows > 1:\n",
    "    axes = axes.flatten()\n",
    "else:\n",
    "    axes = [axes]  # Un seul subplot\n",
    "\n",
    "# Pour chaque bande sélectionnée, créer un affichage\n",
    "for idx, band_num in enumerate(bands_to_display):\n",
    "    # Lire la bande\n",
    "    band = src.read(band_num)  # band_num est basé sur 1, comme l'API de rasterio\n",
    "    band_name = src.descriptions[band_num-1] if band_num-1 < len(src.descriptions) else f\"Bande {band_num}\"\n",
    "    \n",
    "    # Supprimer les valeurs NaN/NoData pour l'affichage\n",
    "    band_display = np.copy(band)\n",
    "    if src.nodata is not None:\n",
    "        band_display[band == src.nodata] = np.nan\n",
    "    \n",
    "    # Calculer les percentiles pour éviter les outliers\n",
    "    valid_data = band_display[~np.isnan(band_display)]\n",
    "    if len(valid_data) > 0:\n",
    "        vmin, vmax = np.nanpercentile(band_display, [2, 98])\n",
    "    else:\n",
    "        vmin, vmax = 0, 1\n",
    "    \n",
    "    # Afficher l'image\n",
    "    im = axes[idx].imshow(band_display, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    axes[idx].set_title(f\"{band_name}\", fontsize=10)\n",
    "    axes[idx].set_xticks([])\n",
    "    axes[idx].set_yticks([])\n",
    "    \n",
    "    # Ajouter une barre de couleur réduite\n",
    "    # Modifier ces paramètres pour ajuster la taille de la barre de couleur\n",
    "    cbar = plt.colorbar(im, ax=axes[idx], fraction=0.035, pad=0.04, shrink=0.7)\n",
    "    cbar.ax.tick_params(labelsize=8)  # Réduire la taille des étiquettes\n",
    "    \n",
    "    # Ajouter des statistiques\n",
    "    mean_val = np.nanmean(band_display)\n",
    "    std_val = np.nanstd(band_display)\n",
    "    axes[idx].text(0.05, 0.95, f\"μ={mean_val:.2f}, σ={std_val:.2f}\", \n",
    "                 transform=axes[idx].transAxes, fontsize=8,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Cacher les subplots vides si le nombre de bandes n'est pas un multiple du nombre de colonnes\n",
    "for i in range(n_bands_to_display, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Aperçu des bandes spectrales sélectionnées\", fontsize=16, y=1.02)\n",
    "plt.subplots_adjust(top=0.95)  # Ajuster pour faire de la place au titre\n",
    "\n",
    "# Sauvegarder la figure\n",
    "output_dir = os.path.join(os.path.dirname(input_file), \"output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(output_dir, \"selected_bands_preview.png\"), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualisation des bandes sélectionnées terminée!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5: Fonctions utilitaires pour le traitement\n",
    "def load_band(image_path, band_index):\n",
    "    \"\"\"Charge une bande spécifique de l'image\"\"\"\n",
    "    with rasterio.open(image_path) as src:\n",
    "        band = src.read(band_index)\n",
    "        profile = src.profile.copy()\n",
    "    return band, profile\n",
    "\n",
    "def save_raster(data, profile, output_path, dtype=None):\n",
    "    \"\"\"Sauvegarde un raster au format GeoTIFF\"\"\"\n",
    "    if dtype:\n",
    "        profile.update(count=1, dtype=dtype, compress='lzw')\n",
    "    \n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(data, 1)\n",
    "    print(f\"Fichier sauvegardé: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7: Classification IsoData avec tous les paramètres\n",
    "\n",
    "###########################################\n",
    "#        CONFIGURATION DE L'ANALYSE       #\n",
    "###########################################\n",
    "\n",
    "# 1. SÉLECTION DES BANDES\n",
    "# Indiquez les numéros des bandes à utiliser (basé sur l'affichage des bandes disponibles)\n",
    "SELECTED_BANDS = [2,4,7,13]\n",
    "\n",
    "# 2. PARAMÈTRES PRINCIPAUX DE L'ALGORITHME ISODATA\n",
    "N_CLUSTERS_MIN = 6    # Nombre minimum de clusters\n",
    "N_CLUSTERS_MAX = 9    # Nombre maximum de clusters\n",
    "MAX_ITERATIONS = 200   # Nombre maximum d'itérations\n",
    "\n",
    "# 3. PARAMÈTRES AVANCÉS DE DIVISION/FUSION\n",
    "MIN_SAMPLES = 40       # Nombre minimum d'échantillons par cluster\n",
    "MAX_STD_DEV = 0.45      # Écart-type maximum dans un cluster avant division\n",
    "MIN_DIST = 0.4        # Distance minimale entre clusters pour fusion\n",
    "MAX_MERGE_PAIRS = 2   # Nombre maximum de paires de clusters à fusionner par itération\n",
    "\n",
    "# 4. PARAMÈTRES DE CONVERGENCE\n",
    "CONVERGENCE_THRESHOLD = 0.03  # Seuil de convergence (% de pixels changeant de cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "###########################################\n",
    "#       EXÉCUTION DE LA CLASSIFICATION    #\n",
    "###########################################\n",
    "\n",
    "print(\"Classification non-supervisée avec IsoData...\")\n",
    "print(f\"Utilisation de {len(SELECTED_BANDS)} bandes: {SELECTED_BANDS}\")\n",
    "\n",
    "# 1. Chargement des bandes sélectionnées\n",
    "bands_data = []\n",
    "band_names = []\n",
    "\n",
    "for i in SELECTED_BANDS:\n",
    "    # Lecture de la bande\n",
    "    band = src.read(i)\n",
    "    name = src.descriptions[i-1] if i-1 < len(src.descriptions) else f\"Bande {i}\"\n",
    "    bands_data.append(band)\n",
    "    band_names.append(name)\n",
    "    print(f\"  ✓ Bande {i}: {name} chargée\")\n",
    "    \n",
    "# 2. Vérification et redimensionnement des bandes\n",
    "reference_shape = bands_data[0].shape\n",
    "for i, band in enumerate(bands_data):\n",
    "    if band.shape != reference_shape:\n",
    "        print(f\"  Redimensionnement de '{band_names[i]}' à {reference_shape}\")\n",
    "        from skimage.transform import resize\n",
    "        bands_data[i] = resize(band, reference_shape, preserve_range=True)\n",
    "\n",
    "# 3. Création du stack de bandes\n",
    "stack = np.stack(bands_data)\n",
    "n_bands, height, width = stack.shape\n",
    "print(f\"Stack de dimensions: {n_bands} bandes x {height} lignes x {width} colonnes\")\n",
    "\n",
    "# 4. Préparation des données pour le clustering\n",
    "data_for_clustering = stack.reshape(n_bands, -1).T\n",
    "valid_pixels = ~np.isnan(data_for_clustering).any(axis=1)\n",
    "valid_data_raw = data_for_clustering[valid_pixels]\n",
    "\n",
    "# 5. Normalisation des données\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "valid_data = scaler.fit_transform(valid_data_raw)\n",
    "print(f\"Données normalisées: {valid_data.shape[0]} pixels valides avec {n_bands} dimensions\")\n",
    "\n",
    "# 6. Initialisation avec KMeans\n",
    "print(f\"Initialisation avec {N_CLUSTERS_MIN} classes...\")\n",
    "kmeans_init = KMeans(\n",
    "    n_clusters=N_CLUSTERS_MIN, \n",
    "    max_iter=10,\n",
    "    init='k-means++',\n",
    "    random_state=42,\n",
    "    n_init='auto'\n",
    ")\n",
    "labels = kmeans_init.fit_predict(valid_data)\n",
    "centers = kmeans_init.cluster_centers_\n",
    "\n",
    "# 7. Algorithme IsoData\n",
    "print(\"Exécution de l'algorithme IsoData...\")\n",
    "iteration = 0\n",
    "n_clusters_current = N_CLUSTERS_MIN\n",
    "previous_labels = None\n",
    "converged = False\n",
    "\n",
    "while iteration < MAX_ITERATIONS and not converged:\n",
    "    iteration += 1\n",
    "    \n",
    "    # 7.1 Assignation des points aux clusters\n",
    "    distances = np.sqrt(((valid_data[:, np.newaxis, :] - centers[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    \n",
    "    # 7.2 Vérification de la convergence\n",
    "    if previous_labels is not None:\n",
    "        changes = np.sum(previous_labels != labels)\n",
    "        change_ratio = changes / len(labels)\n",
    "        if change_ratio < CONVERGENCE_THRESHOLD:\n",
    "            print(f\"Convergence atteinte à l'itération {iteration} ({change_ratio:.4f} < {CONVERGENCE_THRESHOLD})\")\n",
    "            converged = True\n",
    "    \n",
    "    previous_labels = labels.copy()\n",
    "    \n",
    "    # 7.3 Vérifier les clusters vides et les recréer si nécessaire\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    empty_clusters = np.setdiff1d(np.arange(n_clusters_current), unique_labels)\n",
    "    \n",
    "    for empty in empty_clusters:\n",
    "        # Trouver le cluster avec le plus de points et le diviser\n",
    "        largest_cluster = unique_labels[np.argmax(counts)]\n",
    "        largest_indices = np.where(labels == largest_cluster)[0]\n",
    "        \n",
    "        # Prendre les points les plus éloignés comme nouveau centre\n",
    "        largest_points = valid_data[largest_indices]\n",
    "        center_dist = np.sqrt(((largest_points - centers[largest_cluster]) ** 2).sum(axis=1))\n",
    "        farthest_point = largest_indices[np.argmax(center_dist)]\n",
    "        centers[empty] = valid_data[farthest_point]\n",
    "    \n",
    "    # 7.4 Recalculer les centres\n",
    "    for i in range(n_clusters_current):\n",
    "        if i in unique_labels:\n",
    "            cluster_points = valid_data[labels == i]\n",
    "            centers[i] = np.mean(cluster_points, axis=0)\n",
    "    \n",
    "    # 7.5 Diviser les clusters si nécessaire (variance trop élevée)\n",
    "    if n_clusters_current < N_CLUSTERS_MAX:\n",
    "        clusters_to_split = []\n",
    "        for i in range(n_clusters_current):\n",
    "            if i not in unique_labels or counts[list(unique_labels).index(i)] < MIN_SAMPLES:\n",
    "                continue\n",
    "                \n",
    "            cluster_points = valid_data[labels == i]\n",
    "            std_devs = np.std(cluster_points, axis=0)\n",
    "            \n",
    "            if np.any(std_devs > MAX_STD_DEV) and n_clusters_current < N_CLUSTERS_MAX:\n",
    "                clusters_to_split.append((i, std_devs))\n",
    "        \n",
    "        # Trier les clusters par écart-type maximal (diviser d'abord les plus variables)\n",
    "        clusters_to_split.sort(key=lambda x: np.max(x[1]), reverse=True)\n",
    "        \n",
    "        # Diviser les clusters\n",
    "        for i, std_devs in clusters_to_split:\n",
    "            if n_clusters_current >= N_CLUSTERS_MAX:\n",
    "                break\n",
    "                \n",
    "            # Diviser le cluster selon l'axe à plus grande variance\n",
    "            max_var_axis = np.argmax(std_devs)\n",
    "            std_dev = std_devs[max_var_axis]\n",
    "            \n",
    "            # Calculer deux nouveaux centres\n",
    "            centers = np.vstack([centers, centers[i] + np.array([0.5 * std_dev if j == max_var_axis else 0 for j in range(n_bands)])])\n",
    "            centers[i] = centers[i] - np.array([0.5 * std_dev if j == max_var_axis else 0 for j in range(n_bands)])\n",
    "            \n",
    "            n_clusters_current += 1\n",
    "            print(f\"  Cluster {i} divisé selon l'axe {max_var_axis} (std={std_dev:.4f}) → {n_clusters_current} clusters\")\n",
    "    \n",
    "    # 7.6 Fusionner des clusters si nécessaire (trop proches)\n",
    "    if n_clusters_current > N_CLUSTERS_MIN:\n",
    "        center_distances = np.sqrt(((centers[:, np.newaxis, :] - centers[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "        np.fill_diagonal(center_distances, np.inf)  # Ne pas comparer un cluster avec lui-même\n",
    "        \n",
    "        merge_count = 0\n",
    "        while merge_count < MAX_MERGE_PAIRS and n_clusters_current > N_CLUSTERS_MIN:\n",
    "            min_dist_idx = np.unravel_index(np.argmin(center_distances), center_distances.shape)\n",
    "            min_dist_value = center_distances[min_dist_idx]\n",
    "            \n",
    "            if min_dist_value > MIN_DIST:\n",
    "                break  # Pas de clusters assez proches pour fusion\n",
    "                \n",
    "            i, j = min_dist_idx\n",
    "            # Fusionner les clusters i et j\n",
    "            weights = np.array([counts[list(unique_labels).index(i)] if i in unique_labels else 0, \n",
    "                              counts[list(unique_labels).index(j)] if j in unique_labels else 0])\n",
    "            \n",
    "            if np.sum(weights) > 0:\n",
    "                centers[i] = (centers[i] * weights[0] + centers[j] * weights[1]) / np.sum(weights)\n",
    "                \n",
    "            # Supprimer le cluster j\n",
    "            centers = np.delete(centers, j, axis=0)\n",
    "            labels[labels == j] = i\n",
    "            labels[labels > j] -= 1  # Réindexer les labels supérieurs\n",
    "            \n",
    "            # Mettre à jour le nombre de clusters\n",
    "            n_clusters_current -= 1\n",
    "            print(f\"  Clusters {i} et {j} fusionnés (distance={min_dist_value:.4f}) → {n_clusters_current} clusters\")\n",
    "            \n",
    "            # Mettre à jour la matrice de distances\n",
    "            center_distances = np.sqrt(((centers[:, np.newaxis, :] - centers[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "            np.fill_diagonal(center_distances, np.inf)\n",
    "            \n",
    "            merge_count += 1\n",
    "    \n",
    "    print(f\"Itération {iteration}: {n_clusters_current} clusters\")\n",
    "\n",
    "# 8. Reconstruction de l'image classifiée finale\n",
    "clusters = labels\n",
    "classification = np.zeros(height * width, dtype=np.uint8)\n",
    "classification[valid_pixels] = clusters + 1  # Classes commencent à 1\n",
    "classification = classification.reshape(height, width)\n",
    "\n",
    "# 9. Sauvegarder la classification\n",
    "output_dir = os.path.join(os.path.dirname(input_file), \"output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class_path = os.path.join(output_dir, \"classification_isodata.tif\")\n",
    "profile_out = src.profile.copy()\n",
    "profile_out.update(count=1, dtype=rasterio.uint8, nodata=0)\n",
    "\n",
    "with rasterio.open(class_path, 'w', **profile_out) as dst:\n",
    "    dst.write(classification, 1)\n",
    "\n",
    "print(f\"\\nClassification terminée avec {n_clusters_current} classes\")\n",
    "print(f\"Résultat sauvegardé dans: {class_path}\")\n",
    "\n",
    "# 10. Afficher des statistiques sur les classes\n",
    "unique_classes, class_counts = np.unique(classification, return_counts=True)\n",
    "print(\"\\nStatistiques des classes:\")\n",
    "for i, (cls, count) in enumerate(zip(unique_classes[1:], class_counts[1:])):  # Ignorer la classe 0 (non classé)\n",
    "    percentage = (count / np.sum(class_counts[1:])) * 100\n",
    "    print(f\"  Classe {cls}: {count} pixels ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 11. Fusion des classes 4 et 7 pour la carte d'occupation du sol\n",
    "print(\"\\nCréation d'une version avec fusion des classes 4 et 7 (bâtiments)...\")\n",
    "classification_merged = classification.copy()\n",
    "classification_merged[classification == 7] = 4\n",
    "\n",
    "# Sauvegarder la classification fusionnée\n",
    "class_merged_path = os.path.join(output_dir, \"classification_isodata_landcover.tif\")\n",
    "with rasterio.open(class_merged_path, 'w', **profile_out) as dst:\n",
    "    dst.write(classification_merged, 1)\n",
    "\n",
    "# Statistiques après fusion\n",
    "unique_classes_merged, class_counts_merged = np.unique(classification_merged, return_counts=True)\n",
    "print(\"\\nStatistiques des classes après fusion des bâtiments:\")\n",
    "for i, (cls, count) in enumerate(zip(unique_classes_merged[1:], class_counts_merged[1:])):\n",
    "    percentage = (count / np.sum(class_counts_merged[1:])) * 100\n",
    "    class_name = f\"{cls}\"\n",
    "    if cls == 4:\n",
    "        class_name += \" (bâtiments fusionnés)\"\n",
    "    print(f\"  Classe {class_name}: {count} pixels ({percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\nClassification avec fusion sauvegardée dans: {class_merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cellule 8: Visualisation des résultats sans la classe noire\n",
    "print(\"Fusion des classes 4 et 7...\")\n",
    "\n",
    "# Créer une copie de la classification\n",
    "classification_fusionnee = classification.copy()\n",
    "\n",
    "# Remplacer tous les pixels de classe 7 par la classe 4\n",
    "classification_fusionnee[classification == 7] = 4\n",
    "\n",
    "print(f\"Nombre de pixels modifiés: {np.sum(classification == 7)}\")\n",
    "\n",
    "# Utiliser cette classification fusionnée pour la visualisation\n",
    "classification = classification_fusionnee\n",
    "\n",
    "# Recalculer les classes uniques après fusion\n",
    "unique_classes = np.unique(classification)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"Classes après fusion: {unique_classes}\")\n",
    "\n",
    "# Le reste du code de visualisation reste identique\n",
    "plt.figure(figsize=(12, 8)) # Increased width to have space for colorbar\n",
    "\n",
    "# Définir des couleurs personnalisées pour chaque classe en format hexadécimal\n",
    "hex_colors = [\n",
    "    '#FFFFFF00',  # Classe 0: Transparent (au lieu de noir)\n",
    "    '#999999',    # Classe 1: Eau\n",
    "    '#00CC33',    # Classe 2: Forêt\n",
    "    '#33CC00',    # Classe 3: Végétation herbacée\n",
    "    '#FF0000',    # Classe 4: Urbain (fusionnée avec 7)\n",
    "    '#0050CC',    # Classe 5: Sol nu\n",
    "    '#996600',    # Classe 6: Tourbières\n",
    "    '#38761d',    # Classe 8: Végétation arbustive\n",
    "    '#FF6699',    # Classe 9: Milieux humides\n",
    "]\n",
    "\n",
    "# S'assurer d'avoir assez de couleurs\n",
    "if num_classes > len(hex_colors):\n",
    "    # Générer des couleurs supplémentaires si nécessaire\n",
    "    import random\n",
    "    for i in range(len(hex_colors), num_classes):\n",
    "        random_color = '#%02X%02X%02X' % (\n",
    "            random.randint(0, 255),\n",
    "            random.randint(0, 255),\n",
    "            random.randint(0, 255)\n",
    "        )\n",
    "        hex_colors.append(random_color)\n",
    "\n",
    "# Convertir les codes hexadécimaux en couleurs RGBA pour matplotlib (avec transparence)\n",
    "from matplotlib.colors import to_rgba, ListedColormap\n",
    "rgba_colors = [to_rgba(hex_color) for hex_color in hex_colors[:num_classes]]\n",
    "\n",
    "# Créer une colormap personnalisée avec transparence\n",
    "custom_cmap = ListedColormap(rgba_colors)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les classes > 0\n",
    "valid_classes = [c for c in unique_classes if c > 0]\n",
    "print(f\"Classes valides (> 0): {valid_classes}\")\n",
    "\n",
    "# Définir des noms thématiques pour chaque classe\n",
    "class_themes = {\n",
    "    1: \"Eau\",\n",
    "    2: \"Forêt\",\n",
    "    3: \"Végétation herbacée\",\n",
    "    4: \"Urbain\",\n",
    "    5: \"Sol nu\",\n",
    "    6: \"Tourbières\",\n",
    "    8: \"Végétation arbustive\",\n",
    "    9: \"Milieux humides\"\n",
    "}\n",
    "\n",
    "# Créer des étiquettes pour les classes valides avec noms thématiques\n",
    "class_labels = []\n",
    "for i in valid_classes:\n",
    "    if i in class_themes:\n",
    "        if i == 4:\n",
    "            # Spécifier que c'est une fusion pour la classe 4\n",
    "            class_labels.append(f\"{class_themes[i]} (4+7)\")\n",
    "        else:\n",
    "            class_labels.append(f\"{class_themes[i]}\")\n",
    "    else:\n",
    "        class_labels.append(f\"Classe {i}\")\n",
    "\n",
    "# Afficher la classification avec fond transparent pour la classe 0\n",
    "plt.title(\"Classification d'occupation du sol\", fontsize=14)\n",
    "\n",
    "# Ajouter un fond blanc ou une couleur neutre comme base\n",
    "plt.gca().set_facecolor('#EFEFEF')  # Fond gris très clair\n",
    "\n",
    "im = plt.imshow(classification, cmap=custom_cmap, vmin=0, vmax=num_classes-1)\n",
    "\n",
    "# Create a colorbar with labels, positioned outside the plot\n",
    "cax = plt.axes([1, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = plt.colorbar(im, cax=cax, ticks=valid_classes)  # N'afficher que les ticks des classes valides\n",
    "cbar.set_label(\"Types d'occupation du sol\", fontsize=10)\n",
    "cbar.ax.set_yticklabels(class_labels, fontsize=8)\n",
    "\n",
    "# Ajouter plus d'informations à la carte\n",
    "plt.axis('on')\n",
    "plt.grid(False)\n",
    "\n",
    "# Sauvegarder la figure avec fond transparent\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"classification_sans_classe0.png\"), dpi=300, transparent=False)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCarte de classification sans classe 0 (noire) générée avec succès!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule: Statistiques de la classification IsoData\n",
    "\n",
    "# Importations nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Génération des statistiques de classification...\")\n",
    "\n",
    "# 1. Statistiques générales sur les classes\n",
    "unique_classes, class_counts = np.unique(classification, return_counts=True)\n",
    "classes_stats = pd.DataFrame({\n",
    "    'Classe': unique_classes,\n",
    "    'Nombre de pixels': class_counts,\n",
    "    'Pourcentage (%)': (class_counts / np.sum(class_counts) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Exclure la classe 0 (non classifié) si présente\n",
    "if 0 in unique_classes:\n",
    "    valid_pixels_count = np.sum(class_counts) - class_counts[0]\n",
    "    print(f\"Total de pixels classifiés: {valid_pixels_count:,} sur {np.sum(class_counts):,} pixels\")\n",
    "    print(f\"Pourcentage classifié: {(valid_pixels_count / np.sum(class_counts) * 100):.2f}%\")\n",
    "    \n",
    "    # Recalculer les pourcentages sans les pixels non classifiés\n",
    "    classes_stats_valid = classes_stats[classes_stats['Classe'] > 0].copy()\n",
    "    classes_stats_valid['Pourcentage valide (%)'] = (classes_stats_valid['Nombre de pixels'] / valid_pixels_count * 100).round(2)\n",
    "\n",
    "print(\"\\nStatistiques par classe:\")\n",
    "display(classes_stats)\n",
    "\n",
    "# 2. Visualisation de la distribution des classes\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Graphique à barres des classes (exclure la classe 0 si présente)\n",
    "classes_to_plot = classes_stats[classes_stats['Classe'] > 0] if 0 in unique_classes else classes_stats\n",
    "plt.bar(classes_to_plot['Classe'], classes_to_plot['Nombre de pixels'], color='steelblue')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Nombre de pixels')\n",
    "plt.title('Distribution des pixels par classe')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(classes_to_plot['Classe'])\n",
    "\n",
    "# Ajouter les étiquettes de pourcentage sur chaque barre\n",
    "for i, (classe, count, pct) in enumerate(zip(\n",
    "    classes_to_plot['Classe'], \n",
    "    classes_to_plot['Nombre de pixels'],\n",
    "    classes_to_plot['Pourcentage (%)']\n",
    ")):\n",
    "    plt.text(classe, count + (max(classes_to_plot['Nombre de pixels']) * 0.02), \n",
    "             f\"{pct}%\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"classes_distribution.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3. Statistiques spectrales par classe (si les bandes sont disponibles)\n",
    "try:\n",
    "    # Récupérer les données des bandes utilisées pour la classification\n",
    "    band_names = [src.descriptions[i-1] if i-1 < len(src.descriptions) else f\"Bande {i}\" for i in SELECTED_BANDS]\n",
    "    \n",
    "    # Créer un dataframe pour les statistiques spectrales\n",
    "    spectral_stats = []\n",
    "    \n",
    "    # Pour chaque classe, calculer les moyennes et écarts-types des bandes\n",
    "    for class_id in unique_classes:\n",
    "        if class_id == 0:  # Ignorer la classe non classifiée\n",
    "            continue\n",
    "            \n",
    "        # Masque pour cette classe\n",
    "        class_mask = (classification == class_id)\n",
    "        \n",
    "        # Calculer les statistiques pour chaque bande\n",
    "        for band_idx, band_name in zip(SELECTED_BANDS, band_names):\n",
    "            band_data = src.read(band_idx)\n",
    "            \n",
    "            # Statistiques de la bande pour cette classe\n",
    "            band_values = band_data[class_mask]\n",
    "            band_mean = np.nanmean(band_values)\n",
    "            band_std = np.nanstd(band_values)\n",
    "            \n",
    "            spectral_stats.append({\n",
    "                'Classe': class_id,\n",
    "                'Bande': band_name,\n",
    "                'Moyenne': round(float(band_mean), 4),\n",
    "                'Écart-type': round(float(band_std), 4),\n",
    "                'Min': round(float(np.nanmin(band_values)), 4),\n",
    "                'Max': round(float(np.nanmax(band_values)), 4)\n",
    "            })\n",
    "    \n",
    "    # Créer le DataFrame des statistiques spectrales\n",
    "    spectral_df = pd.DataFrame(spectral_stats)\n",
    "    \n",
    "    print(\"\\nStatistiques spectrales par classe:\")\n",
    "    display(spectral_df)\n",
    "    \n",
    "    # Pivot table pour mieux visualiser les moyennes par classe et par bande\n",
    "    pivot_means = spectral_df.pivot(index='Classe', columns='Bande', values='Moyenne')\n",
    "    print(\"\\nMoyennes spectrales par classe:\")\n",
    "    display(pivot_means)\n",
    "    \n",
    "    # Visualisation des signatures spectrales\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Pour chaque classe, tracer la signature spectrale moyenne\n",
    "    for class_id in unique_classes:\n",
    "        if class_id == 0:  # Ignorer la classe non classifiée\n",
    "            continue\n",
    "        \n",
    "        class_data = spectral_df[spectral_df['Classe'] == class_id]\n",
    "        plt.plot(class_data['Bande'], class_data['Moyenne'], 'o-', label=f\"Classe {class_id}\")\n",
    "    \n",
    "    plt.title(\"Signatures spectrales moyennes par classe\")\n",
    "    plt.xlabel(\"Bande spectrale\")\n",
    "    plt.ylabel(\"Valeur moyenne\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"signatures_spectrales.png\"), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Impossible de calculer les statistiques spectrales: {e}\")\n",
    "\n",
    "# 4. Indices de validation du clustering\n",
    "print(\"\\nCalcul des indices de validation du clustering...\")\n",
    "try:\n",
    "    # Reformater pour le calcul des métriques\n",
    "    data_for_metrics = stack.reshape(n_bands, -1).T\n",
    "    valid_pixels = ~np.isnan(data_for_metrics).any(axis=1)\n",
    "    valid_data = data_for_metrics[valid_pixels]\n",
    "    valid_labels = classification.flatten()[valid_pixels]\n",
    "    \n",
    "    # Calculer les indices de validation uniquement pour les pixels classifiés\n",
    "    if 0 in unique_classes:\n",
    "        valid_indices = valid_labels > 0\n",
    "        metrics_data = valid_data[valid_indices]\n",
    "        metrics_labels = valid_labels[valid_indices]\n",
    "    else:\n",
    "        metrics_data = valid_data\n",
    "        metrics_labels = valid_labels\n",
    "    \n",
    "    # Vérifier que chaque cluster a suffisamment d'échantillons\n",
    "    min_samples_per_class = 10  # Minimum requis pour l'indice de silhouette\n",
    "    unique_labels, label_counts = np.unique(metrics_labels, return_counts=True)\n",
    "    small_clusters = [label for label, count in zip(unique_labels, label_counts) if count < min_samples_per_class]\n",
    "    \n",
    "    if len(small_clusters) > 0:\n",
    "        print(f\"Attention: {len(small_clusters)} classes ont moins de {min_samples_per_class} pixels, ce qui peut affecter les métriques de validation.\")\n",
    "        for cluster in small_clusters:\n",
    "            print(f\"  Classe {cluster}: {label_counts[list(unique_labels).index(cluster)]} pixels\")\n",
    "    \n",
    "    # Calculer l'indice de Calinski-Harabasz\n",
    "    try:\n",
    "        calinski_harabasz = metrics.calinski_harabasz_score(metrics_data, metrics_labels)\n",
    "        print(f\"Indice de Calinski-Harabasz: {calinski_harabasz:.4f}\")\n",
    "        print(\"(Une valeur plus élevée indique une meilleure définition des clusters)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Impossible de calculer l'indice de Calinski-Harabasz: {e}\")\n",
    "    \n",
    "    # Calculer l'indice de Davies-Bouldin\n",
    "    try:\n",
    "        davies_bouldin = metrics.davies_bouldin_score(metrics_data, metrics_labels)\n",
    "        print(f\"Indice de Davies-Bouldin: {davies_bouldin:.4f}\")\n",
    "        print(\"(Une valeur plus proche de zéro indique une meilleure séparation des clusters)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Impossible de calculer l'indice de Davies-Bouldin: {e}\")\n",
    "    \n",
    "    # Calculer l'indice de silhouette si possible (pour un échantillon si les données sont trop volumineuses)\n",
    "    try:\n",
    "        # Si les données sont trop volumineuses, échantillonner pour le calcul de silhouette\n",
    "        max_samples_for_silhouette = 10000  # Pour éviter les problèmes de mémoire\n",
    "        \n",
    "        if metrics_data.shape[0] > max_samples_for_silhouette:\n",
    "            print(f\"Échantillonnage de {max_samples_for_silhouette} pixels sur {metrics_data.shape[0]} pour le calcul de silhouette...\")\n",
    "            # Échantillonnage stratifié pour conserver la distribution des classes\n",
    "            from sklearn.model_selection import StratifiedShuffleSplit\n",
    "            \n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=max_samples_for_silhouette, random_state=42)\n",
    "            _, sample_indices = next(sss.split(metrics_data, metrics_labels))\n",
    "            \n",
    "            silhouette_data = metrics_data[sample_indices]\n",
    "            silhouette_labels = metrics_labels[sample_indices]\n",
    "        else:\n",
    "            silhouette_data = metrics_data\n",
    "            silhouette_labels = metrics_labels\n",
    "        \n",
    "        # Vérifier que tous les clusters ont au moins 2 échantillons\n",
    "        unique_labels, counts = np.unique(silhouette_labels, return_counts=True)\n",
    "        has_single_samples = np.any(counts < 2)\n",
    "        \n",
    "        if not has_single_samples:\n",
    "            silhouette_avg = metrics.silhouette_score(silhouette_data, silhouette_labels)\n",
    "            print(f\"Indice de silhouette moyen: {silhouette_avg:.4f}\")\n",
    "            print(\"(Une valeur proche de 1 indique une bonne séparation des clusters)\")\n",
    "        else:\n",
    "            print(\"Impossible de calculer l'indice de silhouette: certaines classes n'ont qu'un seul élément.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Impossible de calculer l'indice de silhouette: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Impossible de calculer les indices de validation: {e}\")\n",
    "\n",
    "# 5. Exporter les statistiques en CSV\n",
    "try:\n",
    "    # Statistiques des classes\n",
    "    classes_stats.to_csv(os.path.join(output_dir, \"classes_statistics.csv\"), index=False)\n",
    "    \n",
    "    # Statistiques spectrales\n",
    "    if 'spectral_df' in locals():\n",
    "        spectral_df.to_csv(os.path.join(output_dir, \"spectral_statistics.csv\"), index=False)\n",
    "        \n",
    "    print(f\"\\nStatistiques exportées dans {output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'exportation des statistiques: {e}\")\n",
    "\n",
    "print(\"\\nAnalyse des statistiques de classification terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Statistiques complémentaires pour la classification IsoData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.ndimage import label, generate_binary_structure\n",
    "\n",
    "print(\"Calcul des statistiques complémentaires...\")\n",
    "\n",
    "#------------------------------------------------------\n",
    "# 1. Matrice de séparabilité entre classes\n",
    "#------------------------------------------------------\n",
    "print(\"\\n1. Analyse de la séparabilité des classes\")\n",
    "\n",
    "# Calculer les centroïdes de chaque classe\n",
    "centroids = {}\n",
    "for class_id in unique_classes:\n",
    "    if class_id == 0:  # Ignorer la classe non classifiée\n",
    "        continue\n",
    "        \n",
    "    # Masque pour cette classe\n",
    "    class_mask = (classification == class_id)\n",
    "    \n",
    "    # Extraire les données pour cette classe\n",
    "    class_data = np.array([src.read(band_idx)[class_mask] for band_idx in SELECTED_BANDS])\n",
    "    \n",
    "    # Calculer le centroïde (moyenne par bande)\n",
    "    centroids[class_id] = np.nanmean(class_data, axis=1)\n",
    "\n",
    "# Calculer la matrice de distances entre centroïdes\n",
    "class_ids = [c for c in unique_classes if c > 0]\n",
    "n_classes = len(class_ids)\n",
    "distance_matrix = np.zeros((n_classes, n_classes))\n",
    "\n",
    "for i, class1 in enumerate(class_ids):\n",
    "    for j, class2 in enumerate(class_ids):\n",
    "        if i != j:\n",
    "            # Distance euclidienne entre les centroïdes\n",
    "            distance_matrix[i, j] = distance.euclidean(centroids[class1], centroids[class2])\n",
    "\n",
    "# Créer un DataFrame pour la matrice de distances\n",
    "distance_df = pd.DataFrame(distance_matrix, \n",
    "                         index=[f'Classe {c}' for c in class_ids],\n",
    "                         columns=[f'Classe {c}' for c in class_ids])\n",
    "\n",
    "print(\"Matrice de distances euclidiennes entre centroïdes des classes:\")\n",
    "display(distance_df)\n",
    "\n",
    "# Visualiser la matrice de distances sous forme de heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(distance_df, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Distances euclidiennes entre les centroïdes des classes')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"distances_centroids.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Calculer les distances minimales, maximales et moyennes\n",
    "min_distances = []\n",
    "for i in range(n_classes):\n",
    "    # Exclure la diagonale (distance à soi-même)\n",
    "    non_zero_distances = distance_matrix[i, :][distance_matrix[i, :] > 0]\n",
    "    if len(non_zero_distances) > 0:\n",
    "        min_distances.append(np.min(non_zero_distances))\n",
    "\n",
    "print(f\"Distance minimale entre deux classes: {np.min(min_distances):.4f}\")\n",
    "print(f\"Distance maximale entre deux classes: {np.max(distance_matrix):.4f}\")\n",
    "print(f\"Distance moyenne entre les classes: {np.mean(distance_matrix[distance_matrix > 0]):.4f}\")\n",
    "\n",
    "# Identifier les paires de classes les plus proches\n",
    "min_dist_idx = np.where(distance_matrix == np.min(min_distances))\n",
    "print(f\"Classes les plus similaires: Classe {class_ids[min_dist_idx[0][0]]} et Classe {class_ids[min_dist_idx[1][0]]}\")\n",
    "\n",
    "# Identifier les paires de classes les plus éloignées\n",
    "max_dist_idx = np.where(distance_matrix == np.max(distance_matrix))\n",
    "print(f\"Classes les plus différentes: Classe {class_ids[max_dist_idx[0][0]]} et Classe {class_ids[max_dist_idx[1][0]]}\")\n",
    "\n",
    "#------------------------------------------------------\n",
    "# 2. Analyse de la variance intra-classe vs inter-classe\n",
    "#------------------------------------------------------\n",
    "print(\"\\n2. Analyse de la variance intra-classe vs inter-classe\")\n",
    "\n",
    "# Calculer la variance intra-classe pour chaque classe\n",
    "intra_class_variance = {}\n",
    "for class_id in class_ids:\n",
    "    # Masque pour cette classe\n",
    "    class_mask = (classification == class_id)\n",
    "    \n",
    "    # Extraire les données pour cette classe\n",
    "    class_data = np.array([src.read(band_idx)[class_mask] for band_idx in SELECTED_BANDS])\n",
    "    class_data = class_data.reshape(len(SELECTED_BANDS), -1).T  # Reshape pour faciliter le calcul\n",
    "    \n",
    "    # Calculer la variance pour chaque bande\n",
    "    var_by_band = np.nanvar(class_data, axis=0)\n",
    "    \n",
    "    # Variance totale (somme des variances par bande)\n",
    "    intra_class_variance[class_id] = np.sum(var_by_band)\n",
    "\n",
    "# Créer un DataFrame pour les variances intra-classe\n",
    "var_df = pd.DataFrame({\n",
    "    'Classe': list(intra_class_variance.keys()),\n",
    "    'Variance intra-classe': list(intra_class_variance.values())\n",
    "})\n",
    "var_df = var_df.sort_values('Variance intra-classe', ascending=False)\n",
    "\n",
    "print(\"Variance intra-classe par classe:\")\n",
    "display(var_df)\n",
    "\n",
    "# Visualiser la variance intra-classe\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(var_df['Classe'].astype(str), var_df['Variance intra-classe'], color='teal')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Variance intra-classe')\n",
    "plt.title('Variance intra-classe par classe')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"variance_intra_classe.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Calculer le ratio variance inter-classe / variance intra-classe (Fisher)\n",
    "# Pour chaque paire de classes\n",
    "fisher_ratios = []\n",
    "class_pairs = []\n",
    "\n",
    "for i, class1 in enumerate(class_ids):\n",
    "    for j, class2 in enumerate(class_ids):\n",
    "        if i < j:  # Pour éviter de calculer deux fois la même paire\n",
    "            # Distance entre les moyennes (variance inter-classe)\n",
    "            inter_var = np.sum((centroids[class1] - centroids[class2])**2)\n",
    "            \n",
    "            # Somme des variances intra-classe\n",
    "            intra_var = intra_class_variance[class1] + intra_class_variance[class2]\n",
    "            \n",
    "            # Ratio de Fisher\n",
    "            if intra_var > 0:\n",
    "                fisher_ratio = inter_var / intra_var\n",
    "                fisher_ratios.append(fisher_ratio)\n",
    "                class_pairs.append((class1, class2))\n",
    "\n",
    "# Créer un DataFrame pour les ratios de Fisher\n",
    "fisher_df = pd.DataFrame({\n",
    "    'Classe 1': [pair[0] for pair in class_pairs],\n",
    "    'Classe 2': [pair[1] for pair in class_pairs],\n",
    "    'Ratio de Fisher': fisher_ratios\n",
    "})\n",
    "fisher_df = fisher_df.sort_values('Ratio de Fisher', ascending=False)\n",
    "\n",
    "print(\"\\nRatio de Fisher (variance inter-classe / variance intra-classe) pour les paires de classes:\")\n",
    "display(fisher_df.head(10))  # Afficher les 10 premières paires\n",
    "\n",
    "print(f\"Ratio de Fisher moyen: {np.mean(fisher_ratios):.4f}\")\n",
    "print(f\"Ratio de Fisher médian: {np.median(fisher_ratios):.4f}\")\n",
    "\n",
    "#------------------------------------------------------\n",
    "# 3. Analyse spatiale des clusters\n",
    "#------------------------------------------------------\n",
    "print(\"\\n3. Analyse spatiale des clusters\")\n",
    "\n",
    "# Créer une structure pour la recherche de composantes connectées\n",
    "structure = generate_binary_structure(2, 2)  # Connectivité à 8 voisins\n",
    "\n",
    "# Statistiques spatiales par classe\n",
    "spatial_stats = []\n",
    "\n",
    "for class_id in class_ids:\n",
    "    # Créer un masque binaire pour cette classe\n",
    "    class_mask = (classification == class_id).astype(np.int32)\n",
    "    \n",
    "    # Identifier les composantes connectées\n",
    "    labeled_array, num_features = label(class_mask, structure=structure)\n",
    "    \n",
    "    # Calculer la superficie de chaque composante (en pixels)\n",
    "    component_sizes = np.bincount(labeled_array.flatten())[1:]  # Exclure l'arrière-plan (0)\n",
    "    \n",
    "    # Statistiques sur les composantes\n",
    "    spatial_stats.append({\n",
    "        'Classe': class_id,\n",
    "        'Nombre de pixels': np.sum(class_mask),\n",
    "        'Nombre de régions': num_features,\n",
    "        'Taille moyenne des régions': np.mean(component_sizes) if num_features > 0 else 0,\n",
    "        'Taille médiane des régions': np.median(component_sizes) if num_features > 0 else 0,\n",
    "        'Plus grande région (pixels)': np.max(component_sizes) if num_features > 0 else 0,\n",
    "        'Plus petite région (pixels)': np.min(component_sizes) if num_features > 0 else 0,\n",
    "        'Fragmentation (régions/pixels)': num_features / np.sum(class_mask) if np.sum(class_mask) > 0 else 0\n",
    "    })\n",
    "\n",
    "# Créer un DataFrame pour les statistiques spatiales\n",
    "spatial_df = pd.DataFrame(spatial_stats)\n",
    "spatial_df = spatial_df.sort_values('Nombre de pixels', ascending=False)\n",
    "\n",
    "print(\"Statistiques spatiales par classe:\")\n",
    "display(spatial_df)\n",
    "\n",
    "# Visualiser la fragmentation des classes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(spatial_df['Classe'].astype(str), spatial_df['Fragmentation (régions/pixels)'], color='purple')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Indice de fragmentation (régions/pixels)')\n",
    "plt.title('Fragmentation spatiale par classe')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"fragmentation_spatiale.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------\n",
    "# 4. Visualisation avancée avec ACP\n",
    "#------------------------------------------------------\n",
    "print(\"\\n4. Visualisation des classes dans l'espace ACP\")\n",
    "\n",
    "# Préparer les données pour l'ACP\n",
    "data_for_pca = []\n",
    "labels_for_pca = []\n",
    "\n",
    "# Échantillonner les pixels pour chaque classe\n",
    "max_samples_per_class = 5000  # Limiter le nombre d'échantillons par classe pour la visualisation\n",
    "for class_id in class_ids:\n",
    "    # Masque pour cette classe\n",
    "    class_mask = (classification == class_id)\n",
    "    \n",
    "    # Extraire les données pour cette classe\n",
    "    class_data = np.array([src.read(band_idx)[class_mask] for band_idx in SELECTED_BANDS])\n",
    "    class_data = class_data.reshape(len(SELECTED_BANDS), -1).T  # Reshape pour faciliter le calcul\n",
    "    \n",
    "    # Filtrer les pixels contenant des NaN\n",
    "    valid_indices = ~np.isnan(class_data).any(axis=1)\n",
    "    valid_data = class_data[valid_indices]\n",
    "    \n",
    "    # Échantillonner (si nécessaire)\n",
    "    if valid_data.shape[0] > max_samples_per_class:\n",
    "        sample_indices = np.random.choice(valid_data.shape[0], max_samples_per_class, replace=False)\n",
    "        valid_data = valid_data[sample_indices]\n",
    "    \n",
    "    # Ajouter aux données pour l'ACP\n",
    "    data_for_pca.append(valid_data)\n",
    "    labels_for_pca.extend([class_id] * valid_data.shape[0])\n",
    "\n",
    "# Combiner les données de toutes les classes\n",
    "if data_for_pca:\n",
    "    all_data = np.vstack(data_for_pca)\n",
    "    labels = np.array(labels_for_pca)\n",
    "    \n",
    "    # Appliquer l'ACP\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(all_data)\n",
    "    \n",
    "    # Créer un DataFrame pour la visualisation\n",
    "    pca_df = pd.DataFrame({\n",
    "        'PC1': pca_result[:, 0],\n",
    "        'PC2': pca_result[:, 1],\n",
    "        'Classe': labels\n",
    "    })\n",
    "    \n",
    "    # Variance expliquée\n",
    "    explained_variance = pca.explained_variance_ratio_ * 100\n",
    "    print(f\"Variance expliquée: PC1 = {explained_variance[0]:.2f}%, PC2 = {explained_variance[1]:.2f}%\")\n",
    "    \n",
    "    # Visualiser les résultats de l'ACP\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Utiliser une palette de couleurs distincte\n",
    "    n_classes = len(class_ids)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, n_classes))\n",
    "    \n",
    "    # Tracer les points par classe\n",
    "    for i, class_id in enumerate(class_ids):\n",
    "        class_data = pca_df[pca_df['Classe'] == class_id]\n",
    "        plt.scatter(class_data['PC1'], class_data['PC2'], c=[colors[i]], \n",
    "                    label=f'Classe {class_id}', alpha=0.7, edgecolors='w', linewidth=0.5)\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({explained_variance[0]:.2f}%)')\n",
    "    plt.ylabel(f'PC2 ({explained_variance[1]:.2f}%)')\n",
    "    plt.title('Visualisation des classes dans l\\'espace ACP')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"classes_acp.png\"), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculer la séparation des classes dans l'espace ACP\n",
    "    class_separations = []\n",
    "    for i, class1 in enumerate(class_ids):\n",
    "        for j, class2 in enumerate(class_ids):\n",
    "            if i < j:  # Pour éviter de calculer deux fois la même paire\n",
    "                # Données ACP pour chaque classe\n",
    "                data1 = pca_df[pca_df['Classe'] == class1][['PC1', 'PC2']].values\n",
    "                data2 = pca_df[pca_df['Classe'] == class2][['PC1', 'PC2']].values\n",
    "                \n",
    "                # Calculer la distance moyenne entre les points des deux classes\n",
    "                if len(data1) > 0 and len(data2) > 0:\n",
    "                    # Échantillonner si trop de points pour éviter des calculs trop lourds\n",
    "                    max_points = 1000\n",
    "                    if len(data1) > max_points:\n",
    "                        indices = np.random.choice(len(data1), max_points, replace=False)\n",
    "                        data1 = data1[indices]\n",
    "                    if len(data2) > max_points:\n",
    "                        indices = np.random.choice(len(data2), max_points, replace=False)\n",
    "                        data2 = data2[indices]\n",
    "                    \n",
    "                    # Calcul des distances\n",
    "                    distances = pairwise_distances(data1, data2)\n",
    "                    avg_distance = np.mean(distances)\n",
    "                    \n",
    "                    class_separations.append({\n",
    "                        'Classe 1': class1,\n",
    "                        'Classe 2': class2,\n",
    "                        'Distance moyenne dans l\\'espace ACP': avg_distance\n",
    "                    })\n",
    "    \n",
    "    # Créer un DataFrame pour les séparations de classes\n",
    "    separation_df = pd.DataFrame(class_separations)\n",
    "    separation_df = separation_df.sort_values('Distance moyenne dans l\\'espace ACP', ascending=False)\n",
    "    \n",
    "    print(\"\\nSéparation des classes dans l'espace ACP:\")\n",
    "    display(separation_df)\n",
    "else:\n",
    "    print(\"Pas assez de données valides pour effectuer l'ACP.\")\n",
    "\n",
    "#------------------------------------------------------\n",
    "# 5. Indice de confusion et d'homogénéité de Shannon\n",
    "#------------------------------------------------------\n",
    "print(\"\\n5. Indices d'homogénéité et d'entropie\")\n",
    "\n",
    "# Calculer l'entropie de Shannon pour chaque classe\n",
    "shannon_entropy = []\n",
    "\n",
    "for class_id in class_ids:\n",
    "    # Masque pour cette classe\n",
    "    class_mask = (classification == class_id)\n",
    "    \n",
    "    # Extraire les données pour cette classe (par exemple la première bande)\n",
    "    band_data = src.read(SELECTED_BANDS[0])[class_mask]\n",
    "    \n",
    "    # Discrétiser les valeurs (pour l'entropie)\n",
    "    bins = 50  # Nombre de bins pour l'histogramme\n",
    "    hist, _ = np.histogram(band_data, bins=bins)\n",
    "    \n",
    "    # Normaliser l'histogramme pour obtenir des probabilités\n",
    "    prob = hist / np.sum(hist)\n",
    "    \n",
    "    # Calculer l'entropie (ignorer les bins vides)\n",
    "    class_entropy = entropy(prob[prob > 0])\n",
    "    \n",
    "    shannon_entropy.append({\n",
    "        'Classe': class_id,\n",
    "        'Entropie de Shannon': class_entropy,\n",
    "        'Nombre de pixels': np.sum(class_mask)\n",
    "    })\n",
    "\n",
    "# Créer un DataFrame pour l'entropie\n",
    "entropy_df = pd.DataFrame(shannon_entropy)\n",
    "entropy_df['Entropie normalisée'] = entropy_df['Entropie de Shannon'] / np.log(bins)\n",
    "entropy_df = entropy_df.sort_values('Entropie normalisée', ascending=False)\n",
    "\n",
    "print(\"Entropie de Shannon par classe (mesure d'homogénéité):\")\n",
    "display(entropy_df)\n",
    "\n",
    "# Visualiser l'entropie par classe\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(entropy_df['Classe'].astype(str), entropy_df['Entropie normalisée'], color='orange')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Entropie normalisée')\n",
    "plt.title('Entropie de Shannon par classe (plus la valeur est basse, plus la classe est homogène)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"entropie_classes.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------\n",
    "# Exportation des résultats\n",
    "#------------------------------------------------------\n",
    "# Exporter les statistiques en CSV\n",
    "try:\n",
    "    # Matrice de distances\n",
    "    distance_df.to_csv(os.path.join(output_dir, \"distance_matrix.csv\"))\n",
    "    \n",
    "    # Variance intra-classe\n",
    "    var_df.to_csv(os.path.join(output_dir, \"intra_class_variance.csv\"), index=False)\n",
    "    \n",
    "    # Ratios de Fisher\n",
    "    fisher_df.to_csv(os.path.join(output_dir, \"fisher_ratios.csv\"), index=False)\n",
    "    \n",
    "    # Statistiques spatiales\n",
    "    spatial_df.to_csv(os.path.join(output_dir, \"spatial_statistics.csv\"), index=False)\n",
    "    \n",
    "    # Entropie\n",
    "    entropy_df.to_csv(os.path.join(output_dir, \"entropy_statistics.csv\"), index=False)\n",
    "    \n",
    "    if 'separation_df' in locals():\n",
    "        # Séparation ACP\n",
    "        separation_df.to_csv(os.path.join(output_dir, \"acp_separation.csv\"), index=False)\n",
    "    \n",
    "    print(f\"\\nStatistiques complémentaires exportées dans {output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'exportation des statistiques complémentaires: {e}\")\n",
    "\n",
    "print(\"\\nAnalyse des statistiques complémentaires terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
